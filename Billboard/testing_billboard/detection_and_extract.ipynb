{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132  Python-3.8.17 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 960M, 4096MiB)\n",
      "Setup complete  (8 CPUs, 15.8 GB RAM, 175.2/238.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision\n",
    "print(torch.__version__,torch.cuda.is_available())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from imutils.object_detection import non_max_suppression\n",
    "\n",
    "from craft_text_detector import Craft\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()\n",
    "\n",
    "import imutils\n",
    "\n",
    "import glob\n",
    "import  traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = r'C://Program Files//Tesseract-OCR//tesseract.exe'\n",
    "\n",
    "os.environ[\"TESSDATA_PREFIX\"] =  \"C://Program Files//Tesseract-OCR//tessdata\"\n",
    "custom_config = r'-c preserve_interword_spaces=5 --oem 3 --psm 4 '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craft Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "craft_detector = Craft( crop_type=\"poly\", cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craft Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CraftModule:\n",
    "    def __init__(self,craft_detector,craft_extractor):\n",
    "        self.craft_extractor = craft_extractor\n",
    "        self.craft_detector = craft_detector\n",
    "        self.boxes = None\n",
    "        self.image = None\n",
    "        self.file = None\n",
    "    \n",
    "    def plot_image(self,img):\n",
    "        plt.axis('off');\n",
    "        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB));\n",
    "        plt.show();\n",
    "\n",
    "    def rotate_image(self):\n",
    "        img = self.image.copy()\n",
    "        image = img.copy()\n",
    "        boxes = self.craft_detector.detect_text(img)['boxes']\n",
    "        angled_boxes = []\n",
    "\n",
    "        # Get angled boxes\n",
    "        for box in boxes:\n",
    "            if len(box) >= 3:  # Ensure at least 3 points for a polygon\n",
    "                # Convert box points to numpy array for easier manipulation\n",
    "                box_points = np.array(box, dtype=np.int32).reshape(-1, 2)\n",
    "\n",
    "                # Calculate the minimum bounding rectangle\n",
    "                rotated_rect = cv2.minAreaRect(box_points)\n",
    "                box_vertices = cv2.boxPoints(rotated_rect)\n",
    "                box_vertices = np.int0(box_vertices)\n",
    "                angled_boxes.append(box_vertices)\n",
    "                cv2.drawContours(img, [box_vertices], 0, (255, 0, 0), 2)\n",
    "        # plt.imshow(img)\n",
    "\n",
    "        try:\n",
    "            # Get largest contour and rotate on basis of that\n",
    "            largest_contour = max(angled_boxes, key=cv2.contourArea)\n",
    "            rows,cols = img.shape[:2]\n",
    "            [vx,vy,x,y] = cv2.fitLine(largest_contour, cv2.DIST_L2,0,0.01,0.01)\n",
    "            lefty = (-x*vy/vx) + y\n",
    "            righty = ((cols-x)*vy/vx)+y\n",
    "\n",
    "            angle_rad = np.arctan2(vy, vx)\n",
    "            angle = np.degrees(angle_rad)[0]\n",
    "\n",
    "            height, width = img.shape[:2]\n",
    "            center = (width // 2, height // 2)\n",
    "            \n",
    "            if angle != 90:\n",
    "                rotation_matrix = cv2.getRotationMatrix2D(center, angle , scale=1.0)\n",
    "                self.image = cv2.warpAffine(self.image, rotation_matrix , (width, height))   \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def detect_text(self,image,file_name):\n",
    "        self.file = file_name\n",
    "        self.image = image\n",
    "        cv2.imwrite(f'tests/{self.file}_0.jpg',image)\n",
    "\n",
    "        # self.plot_image(image)\n",
    "        self.rotate_image()\n",
    "        self.boxes = self.craft_extractor.detect_text(self.image)['boxes']\n",
    "        # self.plot_image(self.image)\n",
    "        # print('------------ Detected Text Regions ------------')\n",
    "        # self.draw_rectangles()\n",
    "        # print('------------ Croped Text Regions ------------')\n",
    "        self.show_cropped_image()\n",
    "\n",
    "    def get_bounding_boxes(self,box):\n",
    "        flat_box = box.flatten()\n",
    "        x_min = round(min([flat_box[x] for x in [0,2,4,6]]))\n",
    "        y_min = round(min([flat_box[y] for y in [1,3,5,7]]))\n",
    "        x_max = round(max([flat_box[x] for x in [0,2,4,6]]))\n",
    "        y_max = round(max([flat_box[y] for y in [1,3,5,7]]))\n",
    "\n",
    "        return x_min,y_min,x_max,y_max\n",
    "    \n",
    "    # def draw_rectangles(self):\n",
    "    #     image_rect = self.image.copy()\n",
    "    #     for box in self.boxes:\n",
    "    #         x_min,y_min,w,h = cv2.boundingRect(box)\n",
    "    #         image_rect = cv2.rectangle(image_rect, (x_min,y_min), (x_min + w,y_min + h), (255,0,0), 2)\n",
    "        # self.plot_image(image_rect)\n",
    "            \n",
    "    def show_cropped_image(self):\n",
    "        count = 1\n",
    "        for box in self.boxes:\n",
    "            x_min,y_min,x_max,y_max = self.get_bounding_boxes(box)\n",
    "            roi = self.image[y_min : y_max , x_min : x_max].copy()\n",
    "            roi = cv2.resize(roi, None, fx= 2, fy= 2, interpolation= cv2.INTER_CUBIC)\n",
    "            roi = cv2.medianBlur(roi, 3)\n",
    "            roi_invert = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            # roi_invert = cv2.adaptiveThreshold(roi_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2)\n",
    "            \n",
    "            \n",
    "            # roi_invert = cv2.bitwise_not(roi_gray)\n",
    "            # roi_invert = cv2.resize(roi_invert, None, fx=2, fy=2)\n",
    "            # thresh = cv2.threshold(roi_gray, 127, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "            # edges = cv2.Canny(roi, 27, 255)\n",
    "            # self.plot_image(roi_gray)\n",
    "            # self.plot_image(cv2.bitwise_not(roi_gray))\n",
    "\n",
    "\n",
    "            # contour, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "            # print(len(contour))\n",
    "            \n",
    "            # text_english_t = pytesseract.image_to_string(thresh,lang = 'eng', config = custom_config)\n",
    "            # text_nepali_t = pytesseract.image_to_string(thresh,lang = 'nep', config = custom_config)\n",
    "            # text_nepali_hin_t = pytesseract.image_to_string(thresh,lang = 'nep+hin', config = custom_config)\n",
    "\n",
    "\n",
    "            text_english = pytesseract.image_to_string(roi_invert,lang = 'eng', config = custom_config)\n",
    "            text_nepali = pytesseract.image_to_string(roi_invert,lang = 'nep', config = custom_config)\n",
    "            text_nepali_hin = pytesseract.image_to_string(roi_invert,lang = 'nep+hin', config = custom_config)\n",
    "\n",
    "            # print(f'English (gray) : {text_english}')\n",
    "            # print(f'Nepali (gray) : {text_nepali}')\n",
    "            # print(f'Nepali + Hindi (gray) : {text_nepali_hin}')\n",
    "\n",
    "            # print(f'English (thresh) : {text_english_t}')\n",
    "            # print(f'Nepali (thresh) : {text_nepali_t}')\n",
    "            # print(f'Nepali + Hindi (thresh) : {text_nepali_hin_t}')\n",
    "\n",
    "            cv2.imwrite(f'tests/{self.file}_{count}.jpg',roi_invert)\n",
    "            \n",
    "            with open(f'tests/{self.file}_{count}.txt', \"w\") as file:\n",
    "                file.write(f'English : {text_english}\\n')\n",
    "                file.write(f'Nepali : {text_nepali}\\n')\n",
    "                file.write(f'Nepali + Hindi : {text_nepali_hin}\\n')\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "            # self.plot_image(roi)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "craft_extractor = Craft( crop_type=\"box\", cuda=True,text_threshold=0.8,link_threshold=0.8,low_text=0.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_type=['JPG','JPEG','PNG','JFIF']\n",
    "images=[]\n",
    "#for copying later\n",
    "filename=[]\n",
    "for format in file_type:\n",
    "    for path in glob.glob(f\"testing3/*.{format}\"):\n",
    "        filename.append(path)\n",
    "        images.append(cv2.imread(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 8 billboards, 406.9ms\n",
      "Speed: 5.0ms preprocess, 406.9ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 287.2ms\n",
      "Speed: 4.0ms preprocess, 287.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 536.5ms\n",
      "Speed: 6.0ms preprocess, 536.5ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 billboards, 482.7ms\n",
      "Speed: 5.0ms preprocess, 482.7ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 6 billboards, 434.8ms\n",
      "Speed: 5.0ms preprocess, 434.8ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 billboards, 521.6ms\n",
      "Speed: 5.0ms preprocess, 521.6ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 406.9ms\n",
      "Speed: 5.0ms preprocess, 406.9ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 297.2ms\n",
      "Speed: 5.0ms preprocess, 297.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 288.2ms\n",
      "Speed: 4.0ms preprocess, 288.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 427.8ms\n",
      "Speed: 6.0ms preprocess, 427.8ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 507.6ms\n",
      "Speed: 5.0ms preprocess, 507.6ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 287.2ms\n",
      "Speed: 5.0ms preprocess, 287.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 443.8ms\n",
      "Speed: 7.0ms preprocess, 443.8ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 547.5ms\n",
      "Speed: 7.0ms preprocess, 547.5ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 billboards, 521.6ms\n",
      "Speed: 5.0ms preprocess, 521.6ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 290.1ms\n",
      "Speed: 5.0ms preprocess, 290.1ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 440.8ms\n",
      "Speed: 6.0ms preprocess, 440.8ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 (no detections), 522.6ms\n",
      "Speed: 6.0ms preprocess, 522.6ms inference, 1.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_12360\\777466639.py\", line 4, in <module>\n",
      "    detected_boards[0].boxes.data = torch.stack([box for box in detected_boards[0].boxes.data])\n",
      "RuntimeError: stack expects a non-empty TensorList\n",
      "\n",
      "0: 320x640 3 billboards, 291.2ms\n",
      "Speed: 5.0ms preprocess, 291.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 289.2ms\n",
      "Speed: 5.0ms preprocess, 289.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 295.2ms\n",
      "Speed: 4.0ms preprocess, 295.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 293.2ms\n",
      "Speed: 4.0ms preprocess, 293.2ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 5 billboards, 308.2ms\n",
      "Speed: 4.0ms preprocess, 308.2ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 293.2ms\n",
      "Speed: 4.0ms preprocess, 293.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 3 billboards, 300.2ms\n",
      "Speed: 5.0ms preprocess, 300.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 (no detections), 306.2ms\n",
      "Speed: 5.0ms preprocess, 306.2ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ghost\\AppData\\Local\\Temp\\ipykernel_12360\\777466639.py\", line 4, in <module>\n",
      "    detected_boards[0].boxes.data = torch.stack([box for box in detected_boards[0].boxes.data])\n",
      "RuntimeError: stack expects a non-empty TensorList\n",
      "\n",
      "0: 320x640 2 billboards, 297.2ms\n",
      "Speed: 5.0ms preprocess, 297.2ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 300.2ms\n",
      "Speed: 4.0ms preprocess, 300.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 320.1ms\n",
      "Speed: 4.0ms preprocess, 320.1ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 409.9ms\n",
      "Speed: 4.0ms preprocess, 409.9ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 301.1ms\n",
      "Speed: 5.0ms preprocess, 301.1ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 2 billboards, 298.2ms\n",
      "Speed: 4.0ms preprocess, 298.2ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 300.2ms\n",
      "Speed: 4.0ms preprocess, 300.2ms inference, 6.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 418.8ms\n",
      "Speed: 6.0ms preprocess, 418.8ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 billboards, 298.2ms\n",
      "Speed: 5.0ms preprocess, 298.2ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 billboards, 413.9ms\n",
      "Speed: 6.0ms preprocess, 413.9ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "for index,image in enumerate(images):\n",
    "    try:\n",
    "        detected_boards = model(image)\n",
    "        detected_boards[0].boxes.data = torch.stack([box for box in detected_boards[0].boxes.data])\n",
    "        boxes = detected_boards[0].boxes.data\n",
    "        file_name = filename[index].replace('\\\\','/')\n",
    "        cv2.imwrite(f'tests/{file_name.split(\"/\")[1]}',image)\n",
    "        count = 0\n",
    "        for box in boxes:\n",
    "            x1,y1,x2,y2 = int(box[0]),int(box[1]),int(box[2]),int(box[3])\n",
    "\n",
    "            roi =  image[y1 : y2, x1 : x2]\n",
    "            count += 1\n",
    "            roi_file_name = f'{file_name.split(\"/\")[1].split(\".\")[0]}_{count}'\n",
    "            CraftModule(craft_detector,craft_extractor).detect_text(roi,roi_file_name)\n",
    "    except Exception:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "galli_maps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

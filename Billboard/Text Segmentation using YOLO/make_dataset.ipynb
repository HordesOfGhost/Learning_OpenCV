{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.132  Python-3.8.17 torch-2.0.1 CUDA:0 (NVIDIA GeForce GTX 960M, 4096MiB)\n",
      "Setup complete  (8 CPUs, 15.8 GB RAM, 178.4/238.5 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import time \n",
    "\n",
    "from craft_text_detector import Craft\n",
    "import numpy as np\n",
    "\n",
    "import traceback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('billboard_500.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.axis('off');\n",
    "    plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB));\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "craft_detector = Craft( crop_type=\"poly\", cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter and Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self,input_dir,output_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.images = None\n",
    "        self.filenames = None\n",
    "    \n",
    "    def plot_image(self,image):\n",
    "        plt.axis('off');\n",
    "        plt.imshow(cv2.cvtColor(image,cv2.COLOR_BGR2RGB));\n",
    "        plt.show();\n",
    "    \n",
    "    def read_files(self):\n",
    "        file_type = ['JPG','JPEG','PNG','JFIF']\n",
    "        self.images = []\n",
    "        #for copying later\n",
    "        self.filenames = []\n",
    "        file_count = 0 \n",
    "        for format in file_type:\n",
    "            for path in glob.glob(f\"{self.input_dir}/*.{format}\",recursive=True):\n",
    "\n",
    "                file_name = path.split('\\\\')\n",
    "                self.filenames.append(file_name[-1])\n",
    "            \n",
    "                self.images.append(cv2.imread(path))\n",
    "                \n",
    "                file_count += 1\n",
    "                \n",
    "                clear_output(wait = True)\n",
    "                print(f'Read {file_count} number of images')\n",
    "    \n",
    "    def rotate_padded_image(self,image):\n",
    "        \n",
    "        padding_size = 50 \n",
    "\n",
    "        padded_image = np.zeros((image.shape[0] + 2 * padding_size, image.shape[1] + 2 * padding_size, 3), dtype=np.uint8)\n",
    "        padded_image[padding_size:padding_size + image.shape[0], padding_size:padding_size + image.shape[1]] = image\n",
    "\n",
    "        img = padded_image.copy()\n",
    "        boxes = craft_detector.detect_text(img)['boxes']\n",
    "        angled_boxes = []\n",
    "\n",
    "        # Get angled boxes\n",
    "        for box in boxes:\n",
    "            if len(box) >= 3:  # Ensure at least 3 points for a polygon\n",
    "                # Convert box points to numpy array for easier manipulation\n",
    "                box_points = np.array(box, dtype=np.int32).reshape(-1, 2)\n",
    "\n",
    "                # Calculate the minimum bounding rectangle\n",
    "                rotated_rect = cv2.minAreaRect(box_points)\n",
    "                box_vertices = cv2.boxPoints(rotated_rect)\n",
    "                box_vertices = np.int0(box_vertices)\n",
    "                angled_boxes.append(box_vertices)\n",
    "        #         cv2.drawContours(img_rect, [box_vertices], 0, (255, 0, 0), 2)\n",
    "        # plt.imshow(img_rect)\n",
    "\n",
    "        try:\n",
    "            # Get largest contour and rotate on basis of that\n",
    "            largest_contour = max(angled_boxes, key=cv2.contourArea)\n",
    "            rows,cols = img.shape[:2]\n",
    "            [vx,vy,x,y] = cv2.fitLine(largest_contour, cv2.DIST_L2,0,0.01,0.01)\n",
    "            lefty = (-x*vy/vx) + y\n",
    "            righty = ((cols-x)*vy/vx)+y\n",
    "\n",
    "            angle_rad = np.arctan2(vy, vx)\n",
    "            angle = np.degrees(angle_rad)[0]\n",
    "\n",
    "            height, width = img.shape[:2]\n",
    "            center = (width // 2, height // 2)\n",
    "            \n",
    "            if angle != 90:\n",
    "                rotation_matrix = cv2.getRotationMatrix2D(center, angle , scale=1.0)\n",
    "                img = cv2.warpAffine(img, rotation_matrix , (width, height))   \n",
    "            return img\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def copy(self):\n",
    "        \n",
    "        self.read_files()\n",
    "        copy_count = 0\n",
    "        \n",
    "        for index,image in enumerate(self.images):\n",
    "            predict_image = model(image)\n",
    "            try:\n",
    "                predict_image[0].boxes.data  = torch.stack([box for box in predict_image[0].boxes.data if (box[4] > 0.2)])\n",
    "                billboards_boxes = predict_image[0].boxes.data\n",
    "                if len(billboards_boxes) > 2:\n",
    "                    dummy = 0\n",
    "                    for box in billboards_boxes:\n",
    "                        x_min,y_min,x_max,y_max = int(box[0]),int(box[1]),int(box[2]),int(box[3])\n",
    "                        roi =  image[y_min : y_max, x_min : x_max]\n",
    "                        rotated_roi = self.rotate_padded_image(roi)\n",
    "\n",
    "                        if rotated_roi is not None :\n",
    "                            # self.plot_image(self.images[index])\n",
    "                            dummy += 1\n",
    "                            cv2.imwrite(f'{self.output_dir}/{dummy}_{self.filenames[index]}',rotated_roi)\n",
    "                            copy_count += 1\n",
    "                \n",
    "            except:\n",
    "                print('No Detections')\n",
    "                traceback.print_exc()\n",
    "        print(f'\\n{copy_count} numbers of files made')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 49 number of images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 17 billboards, 257.3ms\n",
      "Speed: 6.0ms preprocess, 257.3ms inference, 10.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\craft_text_detector\\craft_utils.py:415: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  polys = np.array(polys)\n",
      "c:\\Users\\Ghost\\anaconda3\\envs\\galli_maps\\lib\\site-packages\\craft_text_detector\\predict.py:110: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  polys_as_ratio = np.array(polys_as_ratio)\n",
      "\n",
      "0: 320x640 11 billboards, 251.3ms\n",
      "Speed: 5.0ms preprocess, 251.3ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 billboards, 252.3ms\n",
      "Speed: 4.0ms preprocess, 252.3ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 7 billboards, 249.3ms\n",
      "Speed: 5.0ms preprocess, 249.3ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 13 billboards, 485.7ms\n",
      "Speed: 5.0ms preprocess, 485.7ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 11 billboards, 252.3ms\n",
      "Speed: 5.0ms preprocess, 252.3ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 249.3ms\n",
      "Speed: 5.0ms preprocess, 249.3ms inference, 3.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 1 billboard, 262.3ms\n",
      "Speed: 6.0ms preprocess, 262.3ms inference, 4.0ms postprocess per image at shape (1, 3, 320, 640)\n",
      "\n",
      "0: 320x640 4 billboards, 262.3ms\n",
      "Speed: 7.0ms preprocess, 262.3ms inference, 5.0ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    }
   ],
   "source": [
    "MakeDataset(\"testing/\",\"paste/\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def pad_and_rotate(image_path, padding_size, angle):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Create a black image with padding\n",
    "    padded_image = np.zeros((image.shape[0] + 2 * padding_size, image.shape[1] + 2 * padding_size, 3), dtype=np.uint8)\n",
    "\n",
    "    # Copy the original image into the center of the padded image\n",
    "    padded_image[padding_size:padding_size + image.shape[0], padding_size:padding_size + image.shape[1]] = image\n",
    "\n",
    "    # Rotate the padded image\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((padded_image.shape[1] / 2, padded_image.shape[0] / 2), angle, 1)\n",
    "    rotated_image = cv2.warpAffine(padded_image, rotation_matrix, (padded_image.shape[1], padded_image.shape[0]))\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "# Example usage\n",
    "input_image_path = 'testing/patan.png'\n",
    "output_image_path = 'output_image.jpg'\n",
    "padding_size = 50  # Pixels\n",
    "rotation_angle = 30  # Degrees\n",
    "\n",
    "result_image = pad_and_rotate(input_image_path, padding_size, rotation_angle)\n",
    "cv2.imwrite(output_image_path, result_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {}